# Machine Learning 분류

## Supervised Learning(지도 학습)
- 가장 흔한 케이스
- 타깃 값에 입력 데이터를 매핑하는 방법을 학습
- 대부분 분류와 회귀
- 변종도 존재
  - Sequence Generation(시퀀스 생성) : 사진이 주어지면 설명하는 캡션을 생성
  - Syntax tree (구문 트리) : 문장이 주어지면 분해된 구문 트리를 예측
  - Object detection (물체 감지)
  - Image Segmentation(이미지 분할) : 사진이 주어졌을 때 픽셀 단위로 특정 물체에 마스킹

## Unsupervised Learning(비지도 학습)
- 어떤 타겟도 사용하지 않고, 입력 데이터에 대한 흥미로운 변환을 찾음.
- 차원 축소, 클러스터링

## Self-supervised Learning(자기 지도 학습)
- 사람이 만든 레이블을 사용하지 않음
- 학습 과정에 사람이 개입하지 않는 지도 학습
- 레이블이 필요하지만, 보통 경험적인 알고리즘(heuristic algorithm)을 사용해서 입력 데이터로부터 생성

## reinforcement learning(강화 학습)
- 환경에 대한 정보를 받아 보상을 최대화하는 행동을 선택.


# Machine Learning 모델 평가
- train, valid, test 3개의 세트로 나누어 사용
- valid set 에서 모델의 성능을 평가하여 튜닝을 수행.

## hold-out validation
![image](https://cdn-images-1.medium.com/max/1600/1*obKmc_bTKbUFgcgryhaAnA.png)

- 데이터가 적을 때는, 전체 데이터를 통계적으로 대표하지 못할 수 있다.
- seed값을 바꾸었을 때, 모델의 성능이 매우 달라지면 바로 이 문제!

### step
  - train, validation, test set으로 나눈다
  - train에서 훈련, validation에서 평가
  - 모델을 튜닝하고 다시 훈련하고 평가하여 최종 모델 선택
  - 튜닝이 끝나면, test 데이터를 제외한 데이터들로 훈련하여 마지막으로 test데이터에서 최종 평가
  
  
## K-fold cross-validation
![image](https://www.researchgate.net/profile/Fabian_Pedregosa/publication/278826818/figure/fig10/AS:614336141750297@1523480558954/The-technique-of-KFold-cross-validation-illustrated-here-for-the-case-K-4-involves.png)

- 모델의 성능이 데이터 분할에 따라 편차가 클 때 도움

## K-fold cross-validation with shuffling
- K-fold를 여러 번 반복하는 것
- sklearn에 RepeatedKFold, RepeatedStratifiedKFold 를 이용하여 구현 가능
- 비용이 매우 많이 든다

## 기억해야 할 것들!!!
### 대표성 있는 데이터
- 훈련 데이터와 테스트 데이터가 대표성이 있어야 한다
- 0~9로 분류하는 문제에서 순서대로 8:2로 나눈다면, 0-7 / 8-9 이렇게 나뉠 수 있다.
- shuffle을 하는 것이 일반적

### 시간의 방향
- 과거부터 미래를 예측하려고 한다면, 데이터를 무작위로 섞어서는 안 된다
- 반드시, train data에 있는 데이터보다 test data에 있는 모든 데이터가 미래의 것

### 데이터 중복
- 한 데이터셋에 어떤 데이터 포인트가 두 번 이상 등장한다면, train과 valid set에 둘 다 들어가지는 않는지 확인해야 한다.
- 훈련 데이터의 일부로 테스트하는 최악의경우!!!


# preprocessing
## vectorization (벡터화)
- 사운드, 이미지, 텍스트 등 처리해야 할 것이 무엇이든지 -> 텐서로 변환

## normalization (정규화)
- 이미지는 보통 0-255 이므로 255로 나누어서 0-1사이의 값으로
- 일반적으로 비교적 큰 값이나 균일하지 않은 데이터를 신경망에 바로 주입하는 것은 위험
- 작은 값이거나, 균일한 데이터!
- 평균이 0이 되도록 정규화, 표준 편차가 1이 되도록 정규화

## NA 처리
- 일반적으로 0이 사전에 정의된 의미 있는 값이 아니라면 누락된 값을 0으로 입력해도 괜찮다.
- 테스트 데이터에 누락된 값이 포함될 가능성이 있다면, 누락된 값이 있는 훈렴 샘플을 고의적으로 만들어야 한다.


# feature engineering
- 특성을 더 간단한 방식으로 표현하여 문제를 쉽게 만들어준다!
- 예 시계 이미지 -> 바늘의 좌표 -> 각도
- 최근 딥러닝은 신경망이 자동으로 원본 데이터에서 유용한 특성을 추출할 수 있다
- 그럼에도, 데이터의 수가 적거나 적은 자원을 이용해야할 때, FE는 필수적


# overfitting and underfitting
- 최적화와 일반화 사이의 줄다리기
### Optimization
- 가능한 훈련 데이터에서 최고의 성능을 얻자
### Generalization
- 훈련된 모델이 이전에 본 적 없는 데이터에서 얼마나 잘 수행되는지

## Underfitting
- 모델의 성능이 계속 발전될 여지가 있다
## Overfitting
- 훈련 데이터에서 특화된 패턴을 학습하기 시작했다
- 새로운 데이터와 관련성이 적어 잘못된 판단을 하게 만든다

![image](https://cdn-images-1.medium.com/max/1600/1*Y2ahYXQfkLioau03MLTQ1Q.png)

## Solution
- 더 많은 훈련 데이터를 모은다
- 네트워크의 용량(capacity)을 감소시킨다
- 모델이 수용할 수 있는 정보의 양을 조절, 정보에 제약 -> `규제 regularization`
- dropout

## regularization
- `Ocam's razor` : 더 적은 가정이 필요한 설명이 옳을 것이라는 이론
- 모델의 파라미터 수가 클수록(capacity가 클수록) 모델이 기억 용량이 많다 -> 일반화 능력이 없다 -> 오버피팅에 예민하다

  - L1 규제 : 절댓값에 비례하는 비용
  - L2 규제 : 제곱에 비례하는 비용
  
## Dropout
- 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외
- 각 샘플에 대해 뉴런의 일부를 무작위하게 제거하면 뉴런의 부정한 협업을 방지하고 결국 과대적합을 감소시킨다

![image](https://cdn-images-1.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png)

# 보편적인 ML 작업 흐름

### 마지막 층의 활성화 함수
- 네트웤의 출력에 필요한 제한. 회귀 예에서는 마지막 층에 활성화 함수를 사용하지 않는다.
### 손실 함수
- 풀려고 하는 문제의 종류에 적합해야 한다.
- 미니 배치 데이터에서 계산 가능해야 하고, 미분 가능해야 한다.(역전파 알고리즘을 사용하여 계산하기 위해)
- auc와 같은 값은 직접 최적화될 수 없다. 일반적으로 crossentropy가 낮을수록 AUC가 높다고 기대할 수 있다.
### 최적화 설정
- optimizer 와 학습률 선택
- rmsprop기 무난

문제 유형 | 마지막 층의 활성화 함수 | 손실 함수
----|----|----
이진 분류 | sigmoid | binary_crossentropy
단일 레이블 다중 분류 | softmax | categorical_crossentropy
다중 레이블 다중 분류 | sigmoid | binary_crossentropy
임의 값에 대한 회귀 | x | mse
0과 1 사이 값에 대한 회귀 | sigmoid | mse 또는 binary_crossentropy





